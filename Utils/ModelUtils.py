import os
import sys
import torch
from typing import List, Tuple, Optional, Any
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig
)
from peft import PeftModel
sys.path.append("Utils")
from LoggerUtils import LoggerUtils

class ModelUtils:
    """
    æ¨¡å‹åŠ è½½å·¥å…·ç±»ï¼Œæ”¯æŒ HuggingFace å’Œ .bin æ ¼å¼ï¼Œ
    æ”¯æŒå¤š LoRA é€‚é…å™¨ï¼Œæ”¯æŒå¤šç§é‡åŒ–é…ç½®ã€‚
    """

    @staticmethod
    def load_model(
        model_path: str,
        lora_adapters: Optional[List[str]] = None,
        device: str = "cuda:0",
        load_mode: str = "auto",
        quantize_bits: Optional[int] = None,
        use_bf: bool = False
    ) -> Tuple[Any, Any]:
        """
        åŠ è½½è¯­è¨€æ¨¡å‹ï¼Œæ”¯æŒ HuggingFace / bin / LoRA é€‚é…å™¨ / é‡åŒ–é…ç½®ã€‚

        :param model_path: æ¨¡å‹è·¯å¾„
        :param lora_adapters: å¯é€‰çš„ LoRA é€‚é…å™¨è·¯å¾„åˆ—è¡¨
        :param device: åŠ è½½è®¾å¤‡ï¼ˆcuda/cpuï¼‰
        :param load_mode: åŠ è½½æ¨¡å¼ï¼ˆauto/bin/hfï¼‰
        :param quantize_bits: é‡åŒ–ä½æ•°ï¼ˆ4/8/16/32ï¼‰ï¼Œé»˜è®¤ä¸º None
        :param use_bf: æ˜¯å¦ä½¿ç”¨ bfloat16 ç²¾åº¦ï¼ˆé€‚ç”¨äºæŸäº›ç¡¬ä»¶ï¼‰
        :return: (model, tokenizer)
        """
        print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        lora_adapters = lora_adapters or []

        # è‡ªåŠ¨åˆ¤æ–­åŠ è½½æ¨¡å¼
        if load_mode == "auto":
            load_mode = "bin" if model_path.endswith(".bin") else "hf"

        if load_mode == "bin":
            return ModelUtils._load_bin_model(model_path, device)
        elif load_mode == "hf":
            return ModelUtils._load_hf_model(
                model_path,
                lora_adapters,
                device,
                quantize_bits,
                use_bf
            )
        else:
            raise ValueError(f"æœªçŸ¥çš„åŠ è½½æ¨¡å¼: {load_mode}")

    @staticmethod
    def _load_bin_model(model_path: str, device: str) -> Tuple[Any, Any]:
        """åŠ è½½ .bin Checkpoint æ ¼å¼æ¨¡å‹"""
        print("ğŸ“¦ æ£€æµ‹åˆ° `.bin` Checkpointï¼Œç›´æ¥åŠ è½½æ¨¡å‹...")
        checkpoint = torch.load(model_path, map_location="cpu")

        model = checkpoint.get("model")
        tokenizer = checkpoint.get("tokenizer")

        if not (model and tokenizer):
            raise ValueError("Checkpoint æ–‡ä»¶ç¼ºå°‘ model æˆ– tokenizer å¯¹è±¡")

        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        model.to(device)
        print("âœ… `.bin` Checkpoint åŠ è½½å®Œæˆï¼")
        return model.eval(), tokenizer

    @staticmethod
    def _load_hf_model(
        model_path: str,
        lora_adapters: List[str],
        device: str,
        quantize_bits: Optional[int],
        use_bf: bool
    ) -> Tuple[Any, Any]:
        """åŠ è½½ HuggingFace æ ¼å¼æ¨¡å‹ä¸ Tokenizerï¼Œæ”¯æŒé‡åŒ–ä¸ LoRA"""
        print("ğŸŒ ä» HuggingFace åŠ è½½æ¨¡å‹...")

        # åŠ è½½åˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.pad_token_id = tokenizer.eos_token_id

        # è·å–é‡åŒ–é…ç½®
        quant_config, torch_dtype = ModelUtils._get_quant_config(quantize_bits, use_bf)

        # åŠ è½½åŸºç¡€æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            device_map={"":device} if "cuda" in device else "cpu",
            torch_dtype=torch_dtype,
            quantization_config=quant_config,
            trust_remote_code=True
        )

        # åŠ è½½ LoRA é€‚é…å™¨
        if lora_adapters:
            ModelUtils._load_peft_adapters(model, lora_adapters)

        print("âœ… Hugging Face æ¨¡å‹åŠ è½½å®Œæˆï¼")
        return model.eval(), tokenizer

    @staticmethod
    def _get_quant_config(
        quantize_bits: Optional[int],
        use_bf: bool
    ) -> Tuple[Optional[BitsAndBytesConfig], Optional[torch.dtype]]:
        """ç”Ÿæˆé‡åŒ–é…ç½®å™¨å’Œç²¾åº¦"""
        if quantize_bits in {4, 8}:
            print(f"âš™ï¸ å¯ç”¨ {quantize_bits}-bit é‡åŒ–...")
            compute_dtype = torch.bfloat16 if use_bf else torch.float16

            return BitsAndBytesConfig(
                load_in_4bit=(quantize_bits == 4),
                load_in_8bit=(quantize_bits == 8),
                bnb_4bit_compute_dtype=compute_dtype,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            ), None

        # éé‡åŒ–æƒ…å†µä¸‹ï¼Œæ ¹æ®ç²¾åº¦è®¾ç½® dtype
        dtype_map = {
            16: torch.bfloat16 if use_bf else torch.float16,
            32: torch.float32
        }
        torch_dtype = dtype_map.get(quantize_bits, torch.float16)
        print(f"ğŸ§± åŠ è½½éé‡åŒ–æ¨¡å‹ï¼ˆç²¾åº¦: {torch_dtype}ï¼‰...")
        return None, torch_dtype

    @staticmethod
    def _load_peft_adapters(model: Any, adapters: List[str]) -> Any:
        """åŠ è½½å¹¶åˆå¹¶å¤šä¸ª LoRA æ¨¡å‹é€‚é…å™¨"""
        for idx, adapter_path in enumerate(adapters):
            print(f"ğŸ”— åŠ è½½å¹¶åˆå¹¶ LoRA é€‚é…å™¨ [{idx + 1}/{len(adapters)}]: {adapter_path}")
            model = PeftModel.from_pretrained(
                model, 
                adapter_path,
                torch_dtype=torch.float16)
            model = model.merge_and_unload()

        print(f"âœ… å…±åˆå¹¶ {len(adapters)} ä¸ª LoRA é€‚é…å™¨åˆ°åŸºç¡€æ¨¡å‹")
        return model
    
    @staticmethod
    def generate_response(prompt, model, tokenizer, generate_config,add_special_tokens=False):
        """
        ç»™å®š prompt å’Œæ¨¡å‹ï¼Œä½¿ç”¨ç”Ÿæˆé…ç½®ç”Ÿæˆæ¨¡å‹è¾“å‡ºã€‚

        :param prompt: è¾“å…¥ prompt å­—ç¬¦ä¸²
        :param model: åŠ è½½å¥½çš„æ¨¡å‹
        :param tokenizer: å¯¹åº” tokenizer
        :param generate_config: å­—å…¸å½¢å¼çš„ç”Ÿæˆå‚æ•°
                            åŒ…å« input_max_length, output_max_length, do_sample, top_k, top_p ç­‰é¡¹
        :return: æ¨¡å‹è¾“å‡ºå­—ç¬¦ä¸²
        """
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=generate_config["input_max_length"],
            add_special_tokens=add_special_tokens
        ).to(model.device)
        generate_kwargs = {
            "max_new_tokens": generate_config["output_max_length"],
            "pad_token_id":tokenizer.pad_token_id,
            "eos_token_id":tokenizer.eos_token_id,
            "temperature":generate_config["temperature"]
        }
        if generate_config["do_sample"]:
            # ä½¿ç”¨é‡‡æ ·ç­–ç•¥
            generate_kwargs.update({
                "do_sample": True,
                "top_k": generate_config["top_k"],
                "top_p": generate_config["top_p"]
            })
        else:
            # ä½¿ç”¨è´ªå©ªå¼è§£ç ï¼ˆæˆ–è€…åç»­çš„ beam searchï¼‰
            generate_kwargs.update({
                "do_sample": False
                # å¯åœ¨æ­¤æ‰©å±• beam_search å‚æ•°ï¼Œä¾‹å¦‚ num_beams, early_stopping=True
            })
        with torch.inference_mode():
            outputs = model.generate(
                input_ids=inputs.input_ids,
                attention_mask=inputs.attention_mask,
                **generate_kwargs
            )

        # åªæå–ç”Ÿæˆéƒ¨åˆ†ï¼ˆå¿½ç•¥promptè¾“å…¥ï¼‰
        return tokenizer.decode(
            outputs[0][len(inputs.input_ids[0]):],
            skip_special_tokens=True
        ).strip()
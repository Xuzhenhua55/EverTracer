from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import logging

# é…ç½®æ—¥å¿—æ ¼å¼
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S')

device_map = {"": "cpu"}
# é…ç½®è·¯å¾„å‚æ•°
import argparse

# è®¾ç½®å‘½ä»¤è¡Œå‚æ•°
parser = argparse.ArgumentParser(description='Merge LoRA adapter with base model')
parser.add_argument('--base_model_path', type=str, default="/work/models/tiiuae/falcon-7b", help='Path to base model')
parser.add_argument('--adapter_path', type=str, default="", help='Path to LoRA adapter')
parser.add_argument('--output_dir', type=str, default="", help='Output directory for merged model')

args = parser.parse_args()

base_model_path = args.base_model_path
adapter_path = args.adapter_path 
output_dir = args.output_dir


def main():
    try:
        # æ‰“å°å…³é”®å‚æ•°ä¿¡æ¯
        logging.info("=" * 50)
        logging.info("å¼€å§‹æ¨¡å‹åˆå¹¶æµç¨‹")
        logging.info("åŸºç¡€æ¨¡å‹è·¯å¾„: %s", base_model_path)
        logging.info("é€‚é…å™¨è·¯å¾„: %s", adapter_path)
        logging.info("è¾“å‡ºç›®å½•: %s", output_dir)
        logging.info("=" * 50)

        # 1. åŠ è½½åŸºç¡€æ¨¡å‹
        logging.info("æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹...")
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_path, torch_dtype=torch.float16, device_map="cpu")
        logging.info("âœ… åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ | å†…å­˜å ç”¨: %.1fGB",
                     torch.cuda.memory_allocated() / 1e9)

        # 2. åŠ è½½é€‚é…å™¨
        logging.info("æ­£åœ¨åŠ è½½LoRAé€‚é…å™¨...")
        model = PeftModel.from_pretrained(base_model,
                                          adapter_path,
                                          torch_dtype=torch.float16,
                                          device_map="cpu")
        logging.info("âœ… é€‚é…å™¨åŠ è½½å®Œæˆ | å†…å­˜å ç”¨: %.1fGB",
                     torch.cuda.memory_allocated() / 1e9)

        # 3. åˆå¹¶æ¨¡å‹
        logging.info("å¼€å§‹æ¨¡å‹åˆå¹¶...")
        merged_model = model.merge_and_unload(progressbar=True)
        logging.info("âœ… æ¨¡å‹åˆå¹¶å®Œæˆ | å†…å­˜å ç”¨: %.1fGB",
                     torch.cuda.memory_allocated() / 1e9)

        # 4. ä¿å­˜åˆå¹¶æ¨¡å‹
        logging.info("æ­£åœ¨ä¿å­˜åˆå¹¶æ¨¡å‹åˆ° %s...", output_dir)
        merged_model.save_pretrained(output_dir)
        logging.info("âœ… åˆå¹¶æ¨¡å‹ä¿å­˜å®Œæˆ")

        # 5. ä¿å­˜åˆ†è¯å™¨
        logging.info("æ­£åœ¨ä¿å­˜åˆ†è¯å™¨...")
        tokenizer = AutoTokenizer.from_pretrained(base_model_path)
        tokenizer.save_pretrained(output_dir)
        logging.info("âœ… åˆ†è¯å™¨ä¿å­˜å®Œæˆ")

        logging.info("=" * 50)
        logging.info("ğŸ‰ æ‰€æœ‰æ“ä½œæˆåŠŸå®Œæˆï¼")

    except Exception as e:
        logging.error("âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: %s", str(e), exc_info=True)
        raise


if __name__ == "__main__":
    main()

random_seed: 48
model_name: meta-llama/Llama-3-8B
target_model: ../TrainedCheckpoint/Llama-3-8B/EverTracer/xsum/target/100/merge_lora/dolly_en_15k/merge_lora
target_model_bin : ""
reference_model: ../TrainedCheckpoint/Llama-3-8B/lora/sft/EverTracer/xsum/refer/1000/merge_lora
dataset_name: EdinburghNLP/xsum 
dataset_config_name: null
cache_path: ./cache
use_dataset_cache: true
packing: true
calibration: false
add_eos_token: false
add_bos_token: false
pad_token_id: null
attack_kind: stat 
eval_batch_size: 1 
maximum_samples: 100 
block_size: 128
validation_split_percentage: 0.1
preprocessing_num_workers: 1
mask_filling_model_name: google-t5/t5-base
buffer_size: 1
mask_top_p: 1.0
span_length: 2
pct: 0.3 
ceil_pct: false
int8: true
half: false
perturbation_number: 1 # the number of different perturbation strength / position; debugging parameter, should be set to 1 in the regular running.
sample_number: 5 # the number of sampling
train_sta_idx: 0
train_end_idx: 100
eval_sta_idx: 0
eval_end_idx: 1000
attack_data_path: attack
load_attack_data: false # whether to load prepared attack data if existing.
log_file_path: ../TrainedCheckpoint/Llama-3-8B/lora/sft/EverTracer/xsum/target/100/merge_lora/attack-result.log
perturb: false
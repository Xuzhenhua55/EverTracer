add_bos_token: false
add_eos_token: false
attack_data_path: attack
attack_kind: stat
block_size: 128
buffer_size: 1
cache_path: ./cache
calibration: false
ceil_pct: false
dataset_config_name: null
dataset_name: EdinburghNLP/xsum
eval_batch_size: 1
eval_end_idx: 1000
eval_sta_idx: 0
half: false
int8: true
load_attack_data: false
log_file_path: /work/xzh/TrainedCheckpoint/Llama-2-7b-hf/lora/EverTracer/xsum/target/100/checkpoint-240/epoch20-merge/alpaca_data_52k/mia_attack_without_cali.log
mask_filling_model_name: /work/models/google-t5/t5-base
mask_top_p: 1.0
maximum_samples: 100
model_name: /work/models/meta-llama/Llama-2-7b-hf
packing: true
pad_token_id: null
pct: 0.3
perturb: false
perturbation_number: 1
preprocessing_num_workers: 1
random_seed: 48
reference_model: /work/xzh/TrainedCheckpoint/Llama-2-7b-hf/lora/EverTracer/xsum/refer/10000/checkpoint-25000/merge_lora
sample_number: 5
span_length: 2
target_adapter_path: /work/xzh/TrainedCheckpoint/Llama-2-7b-hf/lora/EverTracer/xsum/target/100/checkpoint-240/epoch20-merge/alpaca_data_52k
target_model: /work/xzh/TrainedCheckpoint/Llama-2-7b-hf/lora/EverTracer/xsum/target/100/checkpoint-240/epoch20-merge
target_model_bin: ''
train_end_idx: 100
train_sta_idx: 0
use_dataset_cache: true
validation_split_percentage: 0.1
